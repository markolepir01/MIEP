# House Prices â€“ Advanced Regression Techniques

Ovaj repozitorijum sadrÅ¾i **eksperimente i inference pipeline** za Kaggle takmiÄenje  
[House Prices: Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques).

Cilj projekta bio je da se predvidi **`SalePrice`** kuÄ‡a na osnovu razliÄitih atributa (npr. veliÄina kuÄ‡e, kvalitet materijala, broj soba itd.), primenom viÅ¡e regresionih modela i poreÄ‘enjem njihovih performansi.

---

## Struktura projekta

```
house-prices/
	data/                    # train.csv i test.csv
	models/                  # saÄuvani modeli (*.pkl)
	submissions/             # Kaggle submission fajlovi (*.csv)
	src/
		config.py        # putanje i globalne konstante
		preprocess.py    # imputacija, skaliranje, one-hot encoding
		models.py        # svi modeli (Ridge, Lasso, ElasticNet, RF, XGB, LGBM, CatBoost)
		utils.py         # RMSE log, transformacije
		train.py         # treniranje modela (eksperimenti)
		infer.py         # inference pipeline za generisanje predikcija
	requirements.txt         # biblioteke potrebne za pokretanje
```

---

## Pokretanje projekta

### Instalacija zavisnosti

```bash
pip install -r requirements.txt
```

### Treniranje modela (eksperimenti)

Pokretanje treninga za Å¾eljeni model (npr. CatBoost):

```bash
python src/train.py --model catboost
```

Dostupni modeli:
`ridge`, `lasso`, `elastic`, `rf`, `xgb`, `lgbm`, `catboost`

Model i preprocesor se automatski Äuvaju u folderu `models/`.

### Generisanje predikcija (inference pipeline)

Nakon treniranja, generisanje Kaggle submission fajla:

```bash
python src/infer.py --model catboost
```

Rezultat Ä‡e biti saÄuvan u:
```
submissions/submission_catboost.csv
```

---

## Eksperimenti i rezultati

U okviru projekta testirano je sedam razliÄitih modela.  
Tabela ispod prikazuje proseÄne vrednosti **RMSE (log)** dobijene kroz 5-Fold Cross Validation.

| Model | Mean CV RMSE(log) |
|--------|-------------------|
| Ridge | 0.14680 |
| Lasso | 0.14428 |
| ElasticNet | 0.14285 |
| Random Forest | 0.14483 |
| XGBoost | 0.12730 |
| LightGBM | 0.13357 |
| **CatBoost** | **0.12447** |

**CatBoost** se pokazao kao najbolji model sa najmanjom greÅ¡kom.

---


## ğŸ“„ ZakljuÄak

- Linearni modeli (Ridge, Lasso, ElasticNet) dali su osnovnu taÄnost i posluÅ¾ili kao baseline.
- Ansambl i boosting modeli (Random Forest, XGBoost, LightGBM, CatBoost) znaÄajno su poboljÅ¡ali rezultate.
- **CatBoost** je postigao najbolju taÄnost uz minimalan tuning hiperparametara i jednostavnu implementaciju.
- Rezultati su potvrÄ‘eni na Kaggle leaderboardu.

---